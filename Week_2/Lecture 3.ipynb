{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 : Sigmoid Neurons, Gradient Descent, Feed-forword neural networks(FNNs), Representation power of FNNs\n",
    "* We learn about the arbitairy functions $y = f(x) , where  x\\epsilon R^n$ other than boolean functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module : Sigmoid Neuron\n",
    "\n",
    "> Input : $ X = (x_1,x_2...x_n) \\epsilon R^n$\n",
    "\n",
    "> Output : $ Y = y\\epsilon R$\n",
    "\n",
    "> Q. Can we have a network which can (approximately) represent such functions ? We answer it later but first we learn about Perceptrons and the Singmoid neurons\n",
    "\n",
    "> Perceptron : A  perceptron  will  fire  if  the  weighted  sum  of  its  inputs  is  greater  than  the threshold $(-w0)$.The  thresholding  logic  used  by  a  perceptronis very harsh !\n",
    "\n",
    "> Example : let us return to our problem of deciding whether we will like or dislike a movie Consider that we base our decision only on one input $(x1=criticsRating which lies between 0 and 1)$ If the threshold is $0.5 (w0=−0.5) andw1= 1$ then what would be the decision for a moviewithcriticsRating= 0.51 ?(like)What  about  a  movie  withcriticsRating=0.49 ?(dislike)It seems harsh that we would like a movie withrating 0.51 but not one with a rating of 0.49.This  behavior  is  not  a  characteristic  of  the specific   problem   we   chose   or   the   specific weight and threshold that we choseIt is a characteristic of the perceptron function itself which behaves like a step function There will always be this sudden change in the decision (from 0 to 1) when $sum(wixi)crosses the threshold (-w0)$ For  most  real  world  applications  we  wouldexpect  a  smoother  decision  function  which gradually changes from 0 to 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid Neuron Function :**\n",
    "> Sigmoid neuron function is a smooth i.e continuous and differentiable at each point so the output of the sigmoid neuron function much smoother than the step function.\n",
    "> Logistic Functions is a sigmoid function which has no sharp transition around the Thresold.Also  the  output y is  no  longer  binary  but  a real value between 0 and 1 which can be in-terpreted as a probability Instead  of  a  like/dislike  decision  we  get  the probability of liking the movie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 3.2: A typical Supervised Machine Learning Setup\n",
    "\n",
    "> Previously in Perceptron, we had an algorithm for learn-ing the weights of a perceptron, we also need a  way  of  learning  the  weights  of  a  sigmoid neuron Before we see such an algorithm we will revisit the concept of error/ Loss function.\n",
    "\n",
    ">A single perceptron can not deal with Non linearly classifiable data because it is not linearly separable What does “cannot deal with” mean? What would happen if we use a perceptron model to classify this data? Some points may be misclassified therefor it hard to drive 0 error in most of training cases but we can minimize this error as mini as much as possible.\n",
    "\n",
    "> **Typical Machine Leaning Setup :** \n",
    "1. Data : dataset of our observation for which we going to develop model to solve corresponding problems so data is the prequisite of models\n",
    "2. Model : An approximate mapping or function or relation b/w data points and corresponding labels in case of supervised learning.\n",
    "3. Parameters : Weights are associated to each inputs that defines how importent each inputs are! therefor we tru to learn these weights through minimization of lost function via optimization and updation of weights algorithms.\n",
    "4. Larning Algorithms : An  algorithm  for  learning  the  parameters  (w)  of  themodel (for example, perceptron learning algorithm, gradient descent, etc.)\n",
    "5. Objective / Loss / Error Function : to guide the learning algorithm- the learn-ing algorithm should aim to minimize the loss function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
